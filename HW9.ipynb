{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alex Friedrichsen\n",
    "\n",
    "# Homework 9 Data Mining 395\n",
    "\n",
    "# April 17, 2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "(Comparing Clustering Algorithms)\n",
    "\n",
    "a) Compare and contrast the performance and suitability of three popular clustering algorithms: K-\n",
    "\n",
    "Means Clustering, Hierarchical Clustering, and Density-Based Spatial Clustering of Applications with\n",
    "\n",
    "Noise (DBSCAN). In your report, compare the three algorithms in terms of the following computational aspects:\n",
    "\n",
    "- Time Complexity: Discuss the time complexity of each algorithm and how it might affect the scalability of the algorithm when dealing with large datasets.\n",
    "- Robustness: Explain how each algorithm is robust to outliers or noise in the dataset and how it handles such data points.\n",
    "- Cluster Shape and Size: Describe how each algorithm handles clusters of different shapes and sizes and discuss which algorithm might be more suitable for datasets with irregularly shaped clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer Problem 1\n",
    "\n",
    "Clustering is a an approach to analyzing a data set that involves grouping data points based on their characteristics. It is a type of unsupervised machine learning. It can help find patterns or groups in the data.\n",
    "\n",
    "The time complexity of each algorithm is dependent on n, the number of data points.\n",
    "\n",
    "- K-means time complexity: $O(nki)$, where k is the number of clusters, and I is the number of iterations. Worst case: exponential, and therefore risky for large data sets.\n",
    "- Hierarchical clustering: $O(n^2)$. Slowest.\n",
    "- DBSCAN $O(n*\\log(n))$. Fastest for large data sets.\n",
    "\n",
    "The robustness is the algorithm’s ability to handle noisy data.\n",
    "\n",
    "- K-means is not very robust as it tends to assign outliers to the nearest cluster, despite the large gap.\n",
    "- Hierarchical clustering is relatively robust due to its basis in distance metrics that can deal with noise. As the noise grows to great, though, it will eventually cause incorrect clustering.\n",
    "- DBSCAN is highly robust to outliers because it does not cluster data points that shouldn’t belong to a specific cluster. Uses a density based approach to identify clusters. Requires minimum number of data points to form a cluster.\n",
    "\n",
    "Cluster shape and cluster size refer to the distribution of data points within clusters.\n",
    "\n",
    "- K-means assume spherical, equal size clusters. It is unsuitable for data sets with irregularly shaped clusters. It also requires the choice of centroids, a central data point for each cluster.\n",
    "- Hierarchical clustering can make clusters of varying shapes and sizes due to the process of building its tree based structure. This allows it to capture more complex relationships.\n",
    "- DBSCAN can handle varying shapes and sizes of clusters as well, due to the density-based approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2:\n",
    "Compare and contrast the performance and suitability of three popular clustering algorithms: K-\n",
    "Means Clustering, Hierarchical Clustering, and Density-Based Spatial Clustering of Applications with\n",
    "Noise (DBSCAN). In your report, compare the three algorithms in terms of the following computational\n",
    "aspects:\n",
    "Time Complexity: Discuss the time complexity of each algorithm and how it might affect the scalability\n",
    "of the algorithm when dealing with large datasets.\n",
    "Robustness: Explain how each algorithm is robust to outliers or noise in the dataset and how it handles\n",
    "such data points.\n",
    "Cluster Shape and Size: Describe how each algorithm handles clusters of different shapes and sizes and\n",
    "discuss which algorithm might be more suitable for datasets with irregularly shaped clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "import time\n",
    "import os\n",
    "import psutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to perform hierarchical clustering and save the evaluation scores and execution information to a file\n",
    "def hierarchical_clustering(X, filename):\n",
    "    start_time = time.time()\n",
    "    process = psutil.Process(os.getpid())\n",
    "    hc = AgglomerativeClustering(n_clusters=2).fit(X)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    memory_usage = process.memory_info().rss / 1024 / 1024 # convert from bytes to MB\n",
    "    silhouette = silhouette_score(X, hc.labels_)\n",
    "    calinski = calinski_harabasz_score(X, hc.labels_)\n",
    "    davies = davies_bouldin_score(X, hc.labels_)\n",
    "    with open(filename, 'a') as f:\n",
    "        f.write(\"Hierarchical Clustering Results:\\n\")\n",
    "        f.write(\"Silhouette Score: {}\\n\".format(silhouette))\n",
    "        f.write(\"Calinski-Harabasz Index: {}\\n\".format(calinski))\n",
    "        f.write(\"Davies-Bouldin Index: {}\\n\".format(davies))\n",
    "        f.write(\"Execution Time: {} seconds\\n\".format(execution_time))\n",
    "        f.write(\"Memory Usage: {} MB\\n\".format(memory_usage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to perform k-means clustering and save the evaluation scores and execution information to a file\n",
    "def kmeans_clustering(X, filename):\n",
    "    start_time = time.time()\n",
    "    process = psutil.Process(os.getpid())\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    memory_usage = process.memory_info().rss / 1024 / 1024 # convert from bytes to MB\n",
    "    silhouette = silhouette_score(X, kmeans.labels_)\n",
    "    calinski = calinski_harabasz_score(X, kmeans.labels_)\n",
    "    davies = davies_bouldin_score(X, kmeans.labels_)\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(\"K-Means Clustering Results:\\n\")\n",
    "        f.write(\"Silhouette Score: {}\\n\".format(silhouette))\n",
    "        f.write(\"Calinski-Harabasz Index: {}\\n\".format(calinski))\n",
    "        f.write(\"Davies-Bouldin Index: {}\\n\".format(davies))\n",
    "        f.write(\"Execution Time: {} seconds\\n\".format(execution_time))\n",
    "        f.write(\"Memory Usage: {} MB\\n\".format(memory_usage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbscan_clustering(X, filename):\n",
    "    start_time = time.time()\n",
    "    process = psutil.Process(os.getpid())\n",
    "    n_samples = X.shape[0]\n",
    "    valid_labels = False\n",
    "    while not valid_labels:\n",
    "        eps = np.random.uniform(0.1, 0.5) # adjust range as necessary\n",
    "        min_samples = np.random.randint(2, int(n_samples / 2)) # adjust range as necessary\n",
    "        try:\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples).fit(X)\n",
    "            silhouette = silhouette_score(X, dbscan.labels_)\n",
    "            calinski = calinski_harabasz_score(X, dbscan.labels_)\n",
    "            davies = davies_bouldin_score(X, dbscan.labels_)\n",
    "            valid_labels = len(set(dbscan.labels_)) > 1\n",
    "        except:\n",
    "            pass\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    memory_usage = process.memory_info().rss / 1024 / 1024 # convert from bytes to MB\n",
    "    with open(filename, 'a') as f:\n",
    "        f.write(\"DBSCAN Clustering Results:\\n\")\n",
    "        f.write(\"Eps: {}\\n\".format(eps))\n",
    "        f.write(\"Min Samples: {}\\n\".format(min_samples))\n",
    "        f.write(\"Silhouette Score: {}\\n\".format(silhouette))\n",
    "        f.write(\"Calinski-Harabasz Index: {}\\n\".format(calinski))\n",
    "        f.write(\"Davies-Bouldin Index: {}\\n\".format(davies))\n",
    "        f.write(\"Execution Time: {} seconds\\n\".format(execution_time))\n",
    "        f.write(\"Memory Usage: {} MB\\n\".format(memory_usage))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_pipeline(X, filename):\n",
    "    # Perform K-Means clustering\n",
    "    kmeans_clustering(X, filename)\n",
    "    \n",
    "    # Perform Hierarchical clustering\n",
    "    hierarchical_clustering(X, filename)\n",
    "    \n",
    "    # Perform DBSCAN clustering\n",
    "    dbscan_clustering(X, filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Load the make_moons data set\n",
    "# X, y = make_moons(n_samples=1000, noise=0.05, random_state=0)  \n",
    "\n",
    "\n",
    "# clustering_pipeline(X, \"make_moons_results.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the smartphone data set\n",
    "# # Load data from CSV file\n",
    "# data = pd.read_csv(\"smartphone/train.csv\")\n",
    "\n",
    "\n",
    "# # Extract X and y data\n",
    "# X = data.iloc[:, :-1].values # assuming last column is the target variable\n",
    "# y = data.iloc[:, -1].values\n",
    "# clustering_pipeline(X, \"smartphone_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_dbscan_params(X, eps_range, min_samples_range):\n",
    "    best_silhouette = -1\n",
    "    best_eps = 0\n",
    "    best_min_samples = 0\n",
    "\n",
    "    for eps in eps_range:\n",
    "        for min_samples in min_samples_range:\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "            labels = dbscan.fit_predict(X)\n",
    "            if len(set(labels)) > 1:\n",
    "                silhouette = silhouette_score(X, labels)\n",
    "                if silhouette > best_silhouette:\n",
    "                    best_silhouette = silhouette\n",
    "                    best_eps = eps\n",
    "                    best_min_samples = min_samples\n",
    "\n",
    "    return best_eps, best_min_samples, best_silhouette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Number of labels is 15. Valid values are 2 to n_samples - 1 (inclusive)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alexp\\Documents\\GitHub\\cs395_data_mining\\HW9.ipynb Cell 10\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexp/Documents/GitHub/cs395_data_mining/HW9.ipynb#X13sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m min_samples_range \u001b[39m=\u001b[39m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m10\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexp/Documents/GitHub/cs395_data_mining/HW9.ipynb#X13sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Find optimal values for eps and min_samples\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/alexp/Documents/GitHub/cs395_data_mining/HW9.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m best_eps, best_min_samples, best_silhouette \u001b[39m=\u001b[39m find_optimal_dbscan_params(X, eps_range, min_samples_range)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexp/Documents/GitHub/cs395_data_mining/HW9.ipynb#X13sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Print the optimal hyperparameters and clustering quality\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexp/Documents/GitHub/cs395_data_mining/HW9.ipynb#X13sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mOptimal eps:\u001b[39m\u001b[39m\"\u001b[39m, best_eps)\n",
      "\u001b[1;32mc:\\Users\\alexp\\Documents\\GitHub\\cs395_data_mining\\HW9.ipynb Cell 10\u001b[0m in \u001b[0;36mfind_optimal_dbscan_params\u001b[1;34m(X, eps_range, min_samples_range)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexp/Documents/GitHub/cs395_data_mining/HW9.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m labels \u001b[39m=\u001b[39m dbscan\u001b[39m.\u001b[39mfit_predict(X)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexp/Documents/GitHub/cs395_data_mining/HW9.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m(labels)) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/alexp/Documents/GitHub/cs395_data_mining/HW9.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     silhouette \u001b[39m=\u001b[39m silhouette_score(X, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexp/Documents/GitHub/cs395_data_mining/HW9.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39mif\u001b[39;00m silhouette \u001b[39m>\u001b[39m best_silhouette:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexp/Documents/GitHub/cs395_data_mining/HW9.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         best_silhouette \u001b[39m=\u001b[39m silhouette\n",
      "File \u001b[1;32mc:\\Users\\alexp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\cluster\\_unsupervised.py:117\u001b[0m, in \u001b[0;36msilhouette_score\u001b[1;34m(X, labels, metric, sample_size, random_state, **kwds)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         X, labels \u001b[39m=\u001b[39m X[indices], labels[indices]\n\u001b[1;32m--> 117\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(silhouette_samples(X, labels, metric\u001b[39m=\u001b[39mmetric, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds))\n",
      "File \u001b[1;32mc:\\Users\\alexp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\cluster\\_unsupervised.py:231\u001b[0m, in \u001b[0;36msilhouette_samples\u001b[1;34m(X, labels, metric, **kwds)\u001b[0m\n\u001b[0;32m    229\u001b[0m n_samples \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(labels)\n\u001b[0;32m    230\u001b[0m label_freqs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mbincount(labels)\n\u001b[1;32m--> 231\u001b[0m check_number_of_labels(\u001b[39mlen\u001b[39;49m(le\u001b[39m.\u001b[39;49mclasses_), n_samples)\n\u001b[0;32m    233\u001b[0m kwds[\u001b[39m\"\u001b[39m\u001b[39mmetric\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m metric\n\u001b[0;32m    234\u001b[0m reduce_func \u001b[39m=\u001b[39m functools\u001b[39m.\u001b[39mpartial(\n\u001b[0;32m    235\u001b[0m     _silhouette_reduce, labels\u001b[39m=\u001b[39mlabels, label_freqs\u001b[39m=\u001b[39mlabel_freqs\n\u001b[0;32m    236\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\alexp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\cluster\\_unsupervised.py:33\u001b[0m, in \u001b[0;36mcheck_number_of_labels\u001b[1;34m(n_labels, n_samples)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[39m\"\"\"Check that number of labels are valid.\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \n\u001b[0;32m     24\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39m    Number of samples.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39m1\u001b[39m \u001b[39m<\u001b[39m n_labels \u001b[39m<\u001b[39m n_samples:\n\u001b[1;32m---> 33\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     34\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNumber of labels is \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m. Valid values are 2 to n_samples - 1 (inclusive)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     35\u001b[0m         \u001b[39m%\u001b[39m n_labels\n\u001b[0;32m     36\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Number of labels is 15. Valid values are 2 to n_samples - 1 (inclusive)"
     ]
    }
   ],
   "source": [
    "# Load the dataset from file\n",
    "data = pd.read_csv(\"shuttle-landing-control.data\", header=None)\n",
    "\n",
    "# Replace asterisks with NaN\n",
    "data = data.replace(\"*\", np.nan)\n",
    "\n",
    "# Perform one-hot encoding on categorical variables\n",
    "data = pd.get_dummies(data, columns=[0, 1, 2, 3, 4, 5])\n",
    "\n",
    "# Drop any rows with NaN values\n",
    "data = data.dropna()\n",
    "\n",
    "# Convert data to a NumPy array\n",
    "X = data.to_numpy()\n",
    "\n",
    "\n",
    "eps_range = np.arange(0.1, 1.0, 0.1)\n",
    "min_samples_range = range(1, 10)\n",
    "\n",
    "# Find optimal values for eps and min_samples\n",
    "best_eps, best_min_samples, best_silhouette = find_optimal_dbscan_params(X, eps_range, min_samples_range)\n",
    "\n",
    "\n",
    "# Print the optimal hyperparameters and clustering quality\n",
    "print(\"Optimal eps:\", best_eps)\n",
    "print(\"Optimal min_samples:\", best_min_samples)\n",
    "print(\"Best silhouette score:\", best_silhouette)\n",
    "# clustering_pipeline(X, 'shuttle_landing_results.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results:\n",
    "\n",
    "## make_moons:\n",
    "K-Means Clustering Results:\n",
    "Silhouette Score: 0.48975082695298533\n",
    "Calinski-Harabasz Index: 1481.4753424968233\n",
    "Davies-Bouldin Index: 0.7817364605843775\n",
    "Execution Time: 0.04900717735290527 seconds\n",
    "Memory Usage: 146.68359375 MB\n",
    "Hierarchical Clustering Results:\n",
    "Silhouette Score: 0.4391867809034311\n",
    "Calinski-Harabasz Index: 1087.7713369249986\n",
    "Davies-Bouldin Index: 0.837284087751716\n",
    "Execution Time: 0.029980182647705078 seconds\n",
    "Memory Usage: 146.69140625 MB\n",
    "DBSCAN Clustering Results:\n",
    "Eps: 0.4503519041566185\n",
    "Min Samples: 147\n",
    "Silhouette Score: 0.3060017383266752\n",
    "Calinski-Harabasz Index: 361.7530597734725\n",
    "Davies-Bouldin Index: 2.8816508031309773\n",
    "Execution Time: 0.24503803253173828 seconds\n",
    "Memory Usage: 147.47265625 MB\n",
    "\n",
    "## smartphone:\n",
    "K-Means Clustering Results:\n",
    "Silhouette Score: 0.37196991401903456\n",
    "Calinski-Harabasz Index: 5934.114761241196\n",
    "Davies-Bouldin Index: 1.074520597574652\n",
    "Execution Time: 0.3592417240142822 seconds\n",
    "Memory Usage: 311.51953125 MB\n",
    "Hierarchical Clustering Results:\n",
    "Silhouette Score: 0.3735621062678581\n",
    "Calinski-Harabasz Index: 5765.523209652622\n",
    "Davies-Bouldin Index: 1.0735359209035171\n",
    "Execution Time: 8.188697576522827 seconds\n",
    "Memory Usage: 318.9765625 MB\n",
    "\n",
    "## shuttle-landing-control:\n",
    "K-Means Clustering Results:\n",
    "Silhouette Score: 0.20685539935851324\n",
    "Calinski-Harabasz Index: 5.1140495867768605\n",
    "Davies-Bouldin Index: 1.518699240339859\n",
    "Execution Time: 0.1279003620147705 seconds\n",
    "Memory Usage: 204.6953125 MB\n",
    "Hierarchical Clustering Results:\n",
    "Silhouette Score: 0.19435098907732273\n",
    "Calinski-Harabasz Index: 4.71151515151515\n",
    "Davies-Bouldin Index: 1.6027259104482954\n",
    "Execution Time: 0.1157076358795166 seconds\n",
    "Memory Usage: 205.296875 MB\n",
    "\n",
    "# Discussion\n",
    "For the make_moons data, k-means had the best silhouette score by a small margin, though all 3 algorithms had positive scores indicating a decent clustering.\n",
    "K-means outerformed in the other metrics as well, and both hierarchical and k-means outperformed DBSCAN, except for in memory usage.\n",
    "Smartphone data had the hierarchical clustering slightly ahead of k-means in silhouette score but not enough to be significant. Hierarchical\n",
    "also took more memory and time. For the shuttle-landing-control dataset, the clustering scores were relatively low at around 0.2, indicating poor clustering.\n",
    "\n",
    "DBSCAN did not run for either the shuttle landing control or smartphone datasets. \n",
    "The smartphone dataset was much larger, and I let DBSCAN iterate randomly over eps values between 0.1 and 0.5,\n",
    "in combination with min_sample sizes between 2 and n/2. I let the code run over night for 635 minutes and it still hadn't found a valid clustering for the data.\n",
    "For the shuttle-landing-control data, I believe it was too noisy for DBSCAN to create meaningful clusters.\n",
    "I got the error \"ValueError: Number of labels is 15. Valid values are 2 to n_samples - 1 (inclusive)\" which indicates it was clustering into 15 different classes,\n",
    "which is clearly too many for the data.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "771b2e09279bc50dd058e87d6b1c79418da63e1e1d4caf4728a325dde75439e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
